---
layout:     blogpost
title:      "От MIDI-контроллеров к музыкальному мэппингу. Часть 2: компьютерные музыкальные инструменты"
date:       2012-10-04 23:04:04
categories: blog
author:     "OSCII"
permalink:  /from-midi-controllers-to-musical-mappings-2.html
alias:      /blog/ot-midi-kontrollerov-k-muzykalnomu-meppingu-chast-2-kompyuternye-muzykalnye-instrumenty
---

<p>По&nbsp;сути, любой музыкальный инструмент, созданный с&nbsp;помощью компьютера, состоит из&nbsp;части, принимающей
  данные музыканта, генератора звука и&nbsp;некой программной прослойки между ними, которая преобразует собранные данные
  так, чтобы синтезатор создавал музыкальный результат.</p>
<p>В&nbsp;качестве простейшего примера, разберем структуру обычной <nobr>MIDI-клавиатуры. </nobr> Этот девайс собирает следующие данные: номер ноты, скорость нажатия, послекасание, а&nbsp;также данные с&nbsp;колес модуляции и&nbsp;питча. Номер ноты и&nbsp;pitch-wheel управляют частотой, скорость нажатия&nbsp;&mdash; громкостью, а&nbsp;колесо модуляции и&nbsp;послекасание используются на&nbsp;усмотрению музыканта (см. рисунок ниже).</p>

<p><img class="ht radius" src="https://cloud.githubusercontent.com/assets/1007225/4956184/75f61d1c-669a-11e4-8fde-17b50a11dd63.png" alt="Мэппинги и архитектура обычной MIDI-клавиатуры."/></p>

<p>На&nbsp;картинке отчетливо видна &laquo;слоистая&raquo; архитектура системы:</p>

<ul>
  <li><strong>Input layer.</strong> Этот слой собирает информацию с&nbsp;различных датчиков: трекинг видео, гироскопы,
    акселлерометры, флексометры, разного рода клавиши, ручки, торренты&nbsp;&mdash; все, что генерирует какую-либо
    информацию, может быть использовано для создания музыки.
  </li>

  <li><strong>Mapping layer.</strong> В&nbsp;этом слое происходит обработка и&nbsp;интерпретация входящих данных, их&nbsp;анализ
    и&nbsp;подготовка к&nbsp;использованию со&nbsp;звуковым модулем. Это то&nbsp;самое связующее звено между
    контроллером и&nbsp;синтезатором, от&nbsp;удачной реализации которого зависит выразительность инструмента. Далее в&nbsp;статье
    мы&nbsp;будем рассматривать именно этот слой.
  </li>

  <li><strong>Synthesis layer.</strong> Здесь все очевидно: в&nbsp;данном слое генерируется звук (или музыка).</li>
</ul>

<p>Как правило, никто не&nbsp;мэпит напрямую данные с&nbsp;контроллеров на&nbsp;параметры синтезатора&nbsp;&mdash;
  сначала делается их&nbsp;предварительная обработка. Если мы&nbsp;имеем дело с&nbsp;танцевальным перформансом, то,
  согласитесь, логичнее было&nbsp;бы манипулировать такими параметрами, как направление и&nbsp;скорость движения, поза и&nbsp;жесты
  вместо сырых данных с&nbsp;датчиков (например, ускорение осей акселерометра, размеры и&nbsp;координаты блобов). Если
  мы&nbsp;сможем управлять системой, оперируя такими конкретными терминами, то&nbsp;можно без труда настроить мэппинг
  так, чтобы, например, при резком движении правой руки вверх-вниз, создавался определенный звук, который будет
  изменяться по&nbsp;мере кручения перформера вокруг своей оси.</p>

<p>Сам мэппинг состоит из&nbsp;двух слоев: первый интерпретирует данные,&nbsp;&mdash; например, вычисляет Quantity of&nbsp;Motion
  из&nbsp;данных трекера, или распознавание жестов и&nbsp;поз,&nbsp;&mdash; а&nbsp;во&nbsp;втором уже решается, как
  именно управлять параметрами синтеза, используя полученную информацию.</p>

<p><img src="https://cloud.githubusercontent.com/assets/1007225/4956183/75f61056-669a-11e4-9efa-daa4ba62be26.png" alt="Слоистая архитектура компьютерных музыкальных инструментов."/></p>

<p>Процесс мэппинга осложняется тем, что обычно количество входящих данных и&nbsp;параметров синтеза отличается, причем
  иногда это очень сильно. Вот как&nbsp;бы вы&nbsp;стали управлять аддитивным синтезатором с&nbsp;
<nobr>64-ю
</nobr> гармониками (итого 128&nbsp;параметров, учитывая, что у&nbsp;каждой гармоники можно изменять громкость и&nbsp;частоту) с&nbsp;помощью перчатки виртуальной реальности, вроде
<a href="http://www.vrealities.com/P5.html" target="_blank"
   title="Перчатка виртуальной реальности P5 Glove">этой</a>? Или обратная ситуация: в&nbsp;рамках интерактивной инсталляции по&nbsp;помещению расставлены десятки датчиков. Как использовать их&nbsp;показания для управления звуком?</p>

<p>Очевидно, что в&nbsp;данных случаях подход &laquo;в&nbsp;лоб&raquo; (как, <a
    href="https://www.youtube.com/watch?v=lqxUzhDIdqo" title="Дабстеп, кинект и аблетон делают воблу" target="_blank">например</a>,
  замэпить один входящий параметр и&nbsp;управлять частотой LFO с&nbsp;помощью него) просто не&nbsp;пройдет. Здесь уже
  могут вступать в&nbsp;ход статистика, алгоритмы машинного обучения, которые, например, помогут снизить размерность
  данных, распознать последовательность событий или определить схожесть двух наборов данных.</p>

<p>Однако, в&nbsp;большинстве случаев, когда имеется разумное количество параметров для мэпинга, можно обойтись без
  сложных алгоритмов и&nbsp;использовать простые математические операции.</p>

<h2>Подробнее про музыкальный мэппинг</h2>

<p>Как уже говорилось, выразительность инструмента зависит от&nbsp;того, как будут использоваться параметры контроллеров
  для извлечения звука. Говоря в&nbsp;терминах сабжа, для создания экспрессивного инструмента необходимо выбрать
  правильную <em>стратегию мэппинга</em>. Для этого необходимо знать, как вообще можно связывать данные между собой.
  Если думать о&nbsp;мэппинге, как о&nbsp;наборе связей между входящими данными и&nbsp;параметрами синтеза, то&nbsp;можно
  выделить три основных вида связей:</p>

<ul>
  <li><strong>One-to-one.</strong> Самый простой способ использовать контроллеры. В&nbsp;данном случае один входящий
    параметр управляет только одним параметром синтеза. Такой вид связи наиболее распространен, он&nbsp;очевиден и&nbsp;очень
    легок в&nbsp;использовании. Классические синтезаторы почти всегда используют такой вид мэппинга.
  </li>

  <li><strong>One-to-many (divergent) mapping.</strong> Один входящий параметр контролирует несколько параметров синтеза
    одновременно. Такой подход позволяет управлять системой с&nbsp;большим количеством параметров, однако его точности
    может оказаться недостаточно для экспрессивного исполнения музыкального произведения.
  </li>

  <li><strong>Many-to-one (convergent) mapping.</strong> В&nbsp;данном случае, несколько входящих параметров
    одновременно контролируют один параметр синтеза. Такой вид связей дает наиболее выразительный результат, но&nbsp;он&nbsp;сложен
    для освоения, соответственно, перформеру необходимо несколько больше времени для того, чтобы в&nbsp;достаточной мере
    овладеть инструментом, нежели в&nbsp;случае с&nbsp;предыдущими способами.
  </li>
</ul>

<p>Редко когда возможно использовать (да&nbsp;и&nbsp;надо&nbsp;ли это?) только один вид связей,&nbsp;&mdash; как
  правило, в&nbsp;инструментах имеют место связи типа <em>many-to-many</em> (иногда также встречаются термины <em>few-to-many</em>,
  <em>many-to-few</em>).</p>

<p>Если вы&nbsp;до&nbsp;сих пор не&nbsp;поняли, что из&nbsp;себя представляют различные виды мэппинга, то&nbsp;взгляните
  на&nbsp;картинку:</p>

<p><img src="https://cloud.githubusercontent.com/assets/1007225/4956180/75eba968-669a-11e4-84bc-01c80462eb96.png" alt="Виды музыкального мэппинга."/></p>

<p>Авторы [3] считают, что причина недостатка выразительности электронной музыки при безграничных возможностях синтеза
  (что должно быть странно, не&nbsp;правда&nbsp;ли?) заключается как раз-таки в&nbsp;повсеместно используемом <em>one-to-one</em>
  мэппинге. Оно и&nbsp;понятно, классические синтезаторы в&nbsp;первую очередь создаются для извлечения звука, для чего
  вынесено на&nbsp;панель множество параметров (читай, <em>one-to-one</em> связей), однако, человек просто не&nbsp;может
  управлять ими всеми во&nbsp;время исполнения.</p>

<p>Кроме того, вышесказанное подтверждают эксперименты, проводимые в&nbsp;Йоркском Университете [4], в&nbsp;которых
  изучалась эффективность использования различных интерфейсов при создании музыки.</p>

<p>Для опытов были выбраны три интерфейса:</p>

<ul>
  <li>GUI-слайдеры, управляемых с&nbsp;помощью мышки;</li>
  <li>слайдеры на&nbsp;
    <nobr>MIDI-контроллере,</nobr>
    управляемые пальцами;
  </li>
  <li>комбинированный интерфейс, управление которым осуществляется двумя руками: в&nbsp;одной руке мышка, в&nbsp;другой&nbsp;&mdash;
    слайдеры.
  </li>
</ul>

<p>В&nbsp;первом случае интерфейс представляет собой стандартный способ управления компьютерными интерфейсами, когда
  можно изменять только один параметр одновременно. Во&nbsp;втором случае пользователю уже можно двигать несколько
  слайдеров, однако, каждый из&nbsp;них замэпен только на&nbsp;один параметр. Третий интерфейс чем-то напоминает
  традиционный инструмент со&nbsp;сложной структурой many-to-many мэппинга. [5]</p>

<p>Несколько человек прошли одинаковый набор тестов, им&nbsp;надо было воспроизвести на&nbsp;всех трех инструментах
  несколько аудио-записей, сложность которых увеличивалась от&nbsp;теста к&nbsp;тесту.</p>

<p>Как уже говорилось, в&nbsp;первых двух интерфейсах используется прямые one-to-one мэппинги. Третий использует тот&nbsp;же
  набор железок, но&nbsp;использует их&nbsp;совсем иначе:</p>

<ul>
  <li>во-первых, для того, чтобы появился звук, пользователю необходимо приложить какие-то усилия. В&nbsp;данном случае
    звук появляется только во&nbsp;время движения мыши, громкость пропорциональна скорости ее&nbsp;движения.
  </li>
  <li>во-вторых, в&nbsp;этом интерфейсе имеется только один one-to-one мэппинг&nbsp;&mdash; для панорамирования. Все
    остальные связи сложные, many-to-many.
  </li>
</ul>

<p>Контроллеры использовались следующим образом:</p>

<ul>
  <li>громкость = скорость мыши + нажатие кнопки мыши + среднее от&nbsp;значений двух слайдеров;</li>
  <li>питч = положение мыши по&nbsp;вертикали + скорость движения второго слайдера;</li>
  <li>тембр = положение мыши по&nbsp;горизонтали + разница между двумя слайдерами;</li>
  <li>панорама = позиция первого слайдера.</li>
</ul>

<p>Результат каждого теста прослушивался человеком и&nbsp;компьютером, и&nbsp;на&nbsp;основании их&nbsp;оценок
  выставлялся балл. Кроме того, каждый тестируемый давал небольшое интервью, в&nbsp;котором описывал личные впечатления
  от&nbsp;использования девайсов.</p>

<p>Подробне об&nbsp;эксперименте можно прочитать&nbsp;в [4], здесь&nbsp;же я&nbsp;кратко изложу некоторые выводы,
  сделанные из&nbsp;них [5], касательно интерфейса с&nbsp;many-to-many мэппингами:</p>

<ul>
  <li>он&nbsp;позволяет исполнителю мыслить жестами, а&nbsp;не&nbsp;параметрами;</li>
  <li>результаты тестов с&nbsp;таким интерфейсом постоянно улучшались. Кроме того, используя его, люди легче справлялись
    со&nbsp;сложными заданиями;
  </li>
  <li>инструментом со&nbsp;сложными связями сложнее овладеть.</li>
</ul>

<p>Второй вывод можно интерпретировать следующим образом: инструменты с&nbsp;many-to-many мэппингами позволяют создавать
  более сложный музыкальный результат, чем при использовании других видов связей. Я&nbsp;думаю, это отличный довод в&nbsp;пользу
  того, чтобы создавать свой следующий инструмент, используя сложные связи.</p>

<h2>Программные инструменты для мэппинга</h2>

<p>В&nbsp;большинстве случаев в&nbsp;качестве прослойки между контроллерами и&nbsp;синтезатором используется <a
    href="http://cycling74.com" target="_blank" title="Официальный сайт Cycling'74, разработчиков Max/MSP">Max/MSP</a>
  или <a href="http://puredata.info" target="_blank"
         title="Самое большое онлайн-коммьюнити пользователей PureData в интернете">PureData</a>. Лично я&nbsp;предпочитаю
  первый вариант, так как под Max существуют две классные библиотеки: <a href="http://ftm.ircam.fr/" target="_blank"
                                                                         title="Официальный сайт библиотеки FTM от IRCAM">FTM</a>
  и&nbsp;<a href="http://jamoma.org/" target="_blank" title="Официальный сайт библиотеки Jamoma для Max/MSP">Jamoma</a>.
  Каждая из&nbsp;них заслуживает серии немаленьких статей, поэтому я&nbsp;только кратко опишу, чем они примечательны для
  человека, создающего инструменты.</p>

<p>Начну с&nbsp;FTM. Эта библиотека добавляет новый тип данных в&nbsp;Max&nbsp;&mdash; матрицы. Причем, если Jitter
  больше ориентирован на&nbsp;визуализацию, то&nbsp;FTM является неким аналогом Matlab в&nbsp;максе. Очень хорошо
  оптимизированная работа с&nbsp;матричными кроме эффективной работы с&nbsp;гранулярным и&nbsp;FFT синтезами, открывает
  возможности машинного обучения и&nbsp;статистического анализа входящих данных. Собственно, для таких целей в&nbsp;FTM
  был добавлен набор MnM (Mapping Toolbox), добавляет такие алгоритмы, как linear regression, principal component
  analysis, gaussian mixture models и&nbsp;много чего еще.</p>

<p>Jamoma примечательна тем, что это не&nbsp;совсем библиотека&nbsp;&mdash; это целый фреймворк. С&nbsp;помощью него
  можно создавать огромные патчи, сохраняя при этом полный контроль над всем. Это достигается засчет того, что к&nbsp;любому
  параметру модуля, собранного с&nbsp;поддержкой джамомы, можно получить доступ из&nbsp;любого места патча, притом что
  название этого параметра имеет OSC-like вид, например <code>/synth1/env1/attack</code>. Управляется все это хозяйство
  с&nbsp;помощью CUE-скриптов: в&nbsp;текстовом файле вручную прописываются сцены, а&nbsp;также изменения параметров при
  переходе от&nbsp;одной сцены к&nbsp;другой.</p>

<p>Я&nbsp;часто натыкаюсь на&nbsp;упоминание джамомы при реализации мэппинга описаниях инсталляци (например, <a
    href="https://www.youtube.com/watch?v=s3t8fE7BYm4" title="Инсталляция от Pierre Jodlowski из IRCAM" target="_blank">Grainstick</a>).
  Но&nbsp;лично я&nbsp;считаю, что удобнее создавать связи прямо в&nbsp;максе, а&nbsp;джамому использовать только для
  управления патчем.</p>

<h2>Сложности</h2>

<p>Самой главной сложностью при разработке интерфейсов для работы с&nbsp;интерактивным звуком является&nbsp;то, что нет
  выработанных best practices. Каждый новый проект является исследованием.</p>

<p>Также наверняка придется собирать собственный синтезатор, так как существующие VST заточены под работу с&nbsp;MIDI,
  иными словами, они управляются дискретными событиями от&nbsp;нажатий по&nbsp;клавишам, в&nbsp;то&nbsp;время как
  перформер двигается совсем не&nbsp;дискретно.</p>

<p>Еще для разработки инструментов необходимо либо работать в&nbsp;команде, либо самому обладать достаточно широкими
  знаниями. Например, наверняка нужен человек, который будет программировать контролеры, разрабатывать и&nbsp;собирать
  девайсы, также нужен будет саунд-дизайнер, программист Max/MSP и, может быть, специалист по&nbsp;компьютерному зрению
  (если используется видео трекинг). Плюс еще понадобится программист графики, если проект подразумевает какую-то
  визуализацию. Разумеется, зачастую люди обладают знаниями сразу в&nbsp;нескольких областях, плюс некоторый софт может
  заменить какого-то узкого спеца (например, <a href="http://jmpelletier.com/cvjit/"
                                                title="Библиотека компьютерного зрения для Max/MSP" target="_blank">cv.jit</a>
  может частично заменить человека, занимающегося компьютерным зрением).</p>

<p>Да, и&nbsp;никогда нельзя забывать, что мы&nbsp;занимаемся искусством, хоть и&nbsp;технологичным. Художественный
  результат всегда важнее способа его реализации, но, занимаясь компьютерным искусством, очень просто забыть об&nbsp;этом
  и&nbsp;уйти с&nbsp;головой в&nbsp;алгоритмы. Спецэффекты быстро утомляют зрителей/слушателей.</p>

<h2>Напоследок</h2>

<p>Данная статья зрела у&nbsp;меня в&nbsp;голове месяцев пять, но&nbsp;тема настолько широка, что ее&nbsp;просто не&nbsp;освоить
  за&nbsp;пару постов в&nbsp;блоге. Думаю, их&nbsp;можно воспринимать как ликбез, так как в&nbsp;дальнейшем я&nbsp;буду
  описывать различные технические моменты своей деятельности в&nbsp;рамках <a href="http://cnmrg.org"
                                                                              title="Группа исследования компьютерной и новой музыки — Computer and New Music Research Group"
                                                                              target="_blank">CNMRG</a> и&nbsp;мне
  совсем не&nbsp;хочется разбавлять тематические статьи базовым материалом.</p>

<p>Во&nbsp;избежание холиваров также хочу отметить, что в&nbsp;данной статье, говоря про инструменты, я&nbsp;имел ввиду
  девайсы, которые производят звук с&nbsp;помощью жестов. Ведь есть еще контроллеристы, которые по-другому подходят к&nbsp;созданию
  музыки, и&nbsp;деятельность которых&nbsp;я, наверное, по-настоящему осознал только после пламенных рассказов <a
      href="http://sickaudio.blogspot.com" title="Блог Ивана Осипенко, a.k.a. Ivn_Os" target="_blank">Ivn_Os</a> про <a
      href="http://moldover.com/" title="Легенда контроллеризма" target="_blank">Молдовера</a>.</p>

<p>В&nbsp;общем, надеюсь, что данный материал был/будет полезен. Stay tuned, планируется много чего интересного!</p>

<p>Хочется особо поблагодарить следующих личностей, которым пришлось читать эту статью, когда она была еще в&nbsp;зачаточном
  состоянии: Алексей Олейников, Юрий Дидевич, Brutallo Sapiens (прости, Женя, забыл твою фамилию), Герман Хохлов. Без их&nbsp;фидбэков
  я&nbsp;бы не&nbsp;решился дописать статью на&nbsp;такую тему.</p>

<h2>Референсы</h2>

<div>
  [3] Rovan, J.B. and others. Instrumental Gestural Mapping Strategies as&nbsp;Expressivity Determinants in&nbsp;Computer
    Music Performance. IRCAM. 1997.<br />
  [4] Hunt&nbsp;A., Kirk, R.&nbsp;Mapping Strategies for Musical Performance. University of&nbsp;York.&nbsp;2000.<br />
  [5] Hunt&nbsp;A., Wanderley&nbsp;M., Kirk, R.&nbsp;Towards a&nbsp;Model for Instrumental Mapping in&nbsp;Expert
    Musical Interaction. University of&nbsp;York, IRCAM. 2004.
</div>